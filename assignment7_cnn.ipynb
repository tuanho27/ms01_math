{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mạth - Assignment 7\n",
    "\n",
    "Using Tensorflow and Keras to implement  the following architectures:\n",
    "\n",
    "\n",
    "1. Lenet as shown in the below figure.\n",
    "Do the ablation study with the following hyper-paramters:\n",
    "- Activation Function: Sigmoid vs tanh vs ReLU. \n",
    "- Optimizer: SGD vs Adam\n",
    "- Number of filters (or depth of tensors): (6 x 6 x 16 x 16) vs (3 x 3 x 8 x 8) vs (12 x 12 x 32 x 32)\n",
    "Note that, at each configuration, keep the other parameters unchanged. Moreover, please plot loss functions for train and validation sets. What can we conclude from the above experiments?\n",
    "\n",
    "\n",
    "2. Mini-ResNet as shown in the below figure.\n",
    "Compare the performance of LeNet and Mini-ResNet.\n",
    "\n",
    "\n",
    "Conclusion:\n",
    "- Original Letnet is good with SGD\n",
    "- The accuracy will drop when we decrease the filters (3,8) and increase significant when we increase the filter (12,32)\n",
    "- MiniResnet is not good than Lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8947948572853959721\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Add, Dense, Activation, BatchNormalization\n",
    "from sklearn.utils import shuffle\n",
    "import wandb\n",
    "\n",
    "tf.random.set_seed(2021)\n",
    "from tensorflow.python.client import device_lib \n",
    "import os\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Struct:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "args = {\n",
    "    \"activation\": \"relu\", #tanh, sigmoid, relu\n",
    "    \"optimizer\": \"adam\", #sgd, adam\n",
    "    \"filters\": [12,32], # (6 x 6 x 16 x 16) vs (3 x 3 x 8 x 8) vs (12 x 12 x 32 x 32)\n",
    "    \"model\": \"miniresnet\" #miniresnet, lenet\n",
    "}\n",
    "    \n",
    "params = Struct(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lenet\n",
    "class LeNet(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(params.filters[0], kernel_size=(5, 5),\n",
    "                            padding='valid', activation=params.activation)\n",
    "        self.pooling1 = MaxPooling2D(padding='same')\n",
    "        self.conv2 = Conv2D(params.filters[1], kernel_size=(5, 5),\n",
    "                            padding='valid', activation=params.activation)\n",
    "        self.pooling2 = MaxPooling2D(padding='same')\n",
    "        self.flat = Flatten()\n",
    "        self.fc1 = Dense(120, activation=params.activation)\n",
    "        self.fc2 = Dense(84, activation=params.activation)\n",
    "        self.out = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pooling1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pooling2(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        y = self.out(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "## Mini Resnet\n",
    "class MiniResnet(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(8, kernel_size=(7, 7),strides=(2, 2),padding='same')\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.relu1 = Activation('relu')\n",
    "        self.pool1 = MaxPooling2D(pool_size=(3, 3),strides=(2, 2), padding='same')\n",
    "        self.block1 = [\n",
    "            self._building_block(8) for _ in range(3)\n",
    "        ]\n",
    "        self.conv2 = Conv2D(8,kernel_size=(1, 1), strides=(2, 2))\n",
    "        self.block2 = [\n",
    "            self._building_block(8) for _ in range(4)\n",
    "        ]\n",
    "        self.avg_pool = GlobalAveragePooling2D()\n",
    "        self.fc = Dense(128, activation='relu')\n",
    "        self.out = Dense(10, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        h = self.conv1(x)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu1(h)\n",
    "        h = self.pool1(h)\n",
    "        for block in self.block1:\n",
    "            h = block(h)\n",
    "        h = self.conv2(h)\n",
    "        for block in self.block2:\n",
    "            h = block(h)\n",
    "        h = self.avg_pool(h)\n",
    "        h = self.fc(h)\n",
    "        y = self.out(h)\n",
    "        return y\n",
    "\n",
    "    def _building_block(self, channel_out=64):\n",
    "        return Block(channel_out=channel_out)\n",
    "\n",
    "class Block(Model):\n",
    "    def __init__(self, channel_out=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(channel_out,kernel_size=(3, 3),padding='same')\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.relu1 = Activation('relu')\n",
    "        self.conv2 = Conv2D(channel_out,kernel_size=(3, 3),padding='same')\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.add = Add()\n",
    "        self.relu2 = Activation('relu')\n",
    "\n",
    "    def call(self, x):\n",
    "        h = self.conv1(x)\n",
    "        h = self.bn1(h)\n",
    "        h = self.relu1(h)\n",
    "        h = self.conv2(h)\n",
    "        h = self.bn2(h)\n",
    "        h = self.add([x, h])\n",
    "        y = self.relu2(h)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X_train, X_test):\n",
    "    X_train = X_train / 255.\n",
    "    X_test = X_test / 255.\n",
    "\n",
    "    mean = np.mean(X_train, axis=(0, 1, 2, 3))\n",
    "    std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "    print('mean:', mean, 'std:', std)\n",
    "    X_train = (X_train - mean) / (std + 1e-7)\n",
    "    X_test = (X_test - mean) / (std + 1e-7)\n",
    "    return X_train, X_test\n",
    "\n",
    "def prepare_cifar(x, y):\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    y = tf.cast(y, tf.int32)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.4733630004850874 std: 0.25156892506322026\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.11.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">faithful-brook-19</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/tuanhoict2020/Assignment\" target=\"_blank\">https://wandb.ai/tuanhoict2020/Assignment</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/tuanhoict2020/Assignment/runs/1z3sd29w\" target=\"_blank\">https://wandb.ai/tuanhoict2020/Assignment/runs/1z3sd29w</a><br/>\n",
       "                Run data is saved locally in <code>/home/tuanho/Workspace/ms/math/Assignment/wandb/run-20210727_000005-1z3sd29w</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 2.3706233501434326 acc: 0.1015625\n",
      "0 40 loss: 2.3008153438568115 acc: 0.09443359\n",
      "0 80 loss: 2.2899322509765625 acc: 0.11621094\n",
      "0 120 loss: 2.259572982788086 acc: 0.13896485\n",
      "0 160 loss: 2.068004608154297 acc: 0.17197266\n",
      "test acc: 0.2571\n",
      "1 0 loss: 1.9624017477035522 acc: 0.2109375\n",
      "1 40 loss: 1.9093332290649414 acc: 0.26279297\n",
      "1 80 loss: 1.8682427406311035 acc: 0.27871093\n",
      "1 120 loss: 1.8567272424697876 acc: 0.27246094\n",
      "1 160 loss: 1.8426616191864014 acc: 0.27626953\n",
      "test acc: 0.2976\n",
      "2 0 loss: 1.8378270864486694 acc: 0.26953125\n",
      "2 40 loss: 1.869619369506836 acc: 0.29560548\n",
      "2 80 loss: 1.8038009405136108 acc: 0.30605468\n",
      "2 120 loss: 1.819007158279419 acc: 0.31513673\n",
      "2 160 loss: 1.827231764793396 acc: 0.31572264\n",
      "test acc: 0.3274\n",
      "3 0 loss: 1.814780592918396 acc: 0.265625\n",
      "3 40 loss: 1.74114191532135 acc: 0.3227539\n",
      "3 80 loss: 1.8146960735321045 acc: 0.33398438\n",
      "3 120 loss: 1.7386471033096313 acc: 0.32685548\n",
      "3 160 loss: 1.8090534210205078 acc: 0.33320314\n",
      "test acc: 0.3381\n",
      "4 0 loss: 1.716461420059204 acc: 0.3515625\n",
      "4 40 loss: 1.7075111865997314 acc: 0.3475586\n",
      "4 80 loss: 1.751770257949829 acc: 0.33935547\n",
      "4 120 loss: 1.7105813026428223 acc: 0.3430664\n",
      "4 160 loss: 1.6659750938415527 acc: 0.34511718\n",
      "test acc: 0.3542\n",
      "5 0 loss: 1.6908824443817139 acc: 0.3671875\n",
      "5 40 loss: 1.6588044166564941 acc: 0.34746093\n",
      "5 80 loss: 1.6311299800872803 acc: 0.34589845\n",
      "5 120 loss: 1.6474907398223877 acc: 0.359375\n",
      "5 160 loss: 1.7710957527160645 acc: 0.3586914\n",
      "test acc: 0.3573\n",
      "6 0 loss: 1.7136011123657227 acc: 0.2890625\n",
      "6 40 loss: 1.7187610864639282 acc: 0.35595703\n",
      "6 80 loss: 1.7126460075378418 acc: 0.3623047\n",
      "6 120 loss: 1.7402455806732178 acc: 0.36044922\n",
      "6 160 loss: 1.6897823810577393 acc: 0.36523438\n",
      "test acc: 0.3678\n",
      "7 0 loss: 1.7464282512664795 acc: 0.296875\n",
      "7 40 loss: 1.6513352394104004 acc: 0.37314454\n",
      "7 80 loss: 1.6648268699645996 acc: 0.37197265\n",
      "7 120 loss: 1.6814693212509155 acc: 0.3605469\n",
      "7 160 loss: 1.6913812160491943 acc: 0.37070313\n",
      "test acc: 0.37\n",
      "8 0 loss: 1.7744754552841187 acc: 0.3359375\n",
      "8 40 loss: 1.6615087985992432 acc: 0.37148437\n",
      "8 80 loss: 1.7161736488342285 acc: 0.37822264\n",
      "8 120 loss: 1.6564501523971558 acc: 0.37197265\n",
      "8 160 loss: 1.6661791801452637 acc: 0.38671875\n",
      "test acc: 0.3854\n",
      "9 0 loss: 1.6545417308807373 acc: 0.37109375\n",
      "9 40 loss: 1.6198114156723022 acc: 0.38759765\n",
      "9 80 loss: 1.6821438074111938 acc: 0.37910157\n",
      "9 120 loss: 1.6776843070983887 acc: 0.3876953\n",
      "9 160 loss: 1.6680055856704712 acc: 0.38925782\n",
      "test acc: 0.3913\n",
      "10 0 loss: 1.603238821029663 acc: 0.44140625\n",
      "10 40 loss: 1.5876202583312988 acc: 0.3788086\n",
      "10 80 loss: 1.5479609966278076 acc: 0.39853516\n",
      "10 120 loss: 1.5971037149429321 acc: 0.39384764\n",
      "10 160 loss: 1.5419014692306519 acc: 0.38720703\n",
      "test acc: 0.3878\n",
      "11 0 loss: 1.6190420389175415 acc: 0.37109375\n",
      "11 40 loss: 1.600172519683838 acc: 0.40117186\n",
      "11 80 loss: 1.5781922340393066 acc: 0.40126953\n",
      "11 120 loss: 1.5953538417816162 acc: 0.40302736\n",
      "11 160 loss: 1.5250544548034668 acc: 0.40664062\n",
      "test acc: 0.4035\n",
      "12 0 loss: 1.5714136362075806 acc: 0.4375\n",
      "12 40 loss: 1.568155288696289 acc: 0.40585938\n",
      "12 80 loss: 1.6040074825286865 acc: 0.40703124\n",
      "12 120 loss: 1.5238916873931885 acc: 0.40839845\n",
      "12 160 loss: 1.6174274682998657 acc: 0.41435546\n",
      "test acc: 0.4033\n",
      "13 0 loss: 1.5703096389770508 acc: 0.42578125\n",
      "13 40 loss: 1.6247930526733398 acc: 0.40683594\n",
      "13 80 loss: 1.5580545663833618 acc: 0.4100586\n",
      "13 120 loss: 1.6083240509033203 acc: 0.41386718\n",
      "13 160 loss: 1.5245440006256104 acc: 0.415625\n",
      "test acc: 0.412\n",
      "14 0 loss: 1.595292091369629 acc: 0.42578125\n",
      "14 40 loss: 1.6292191743850708 acc: 0.42128906\n",
      "14 80 loss: 1.6388580799102783 acc: 0.41298828\n",
      "14 120 loss: 1.5648787021636963 acc: 0.41884765\n",
      "14 160 loss: 1.4552971124649048 acc: 0.4239258\n",
      "test acc: 0.4152\n",
      "15 0 loss: 1.5074896812438965 acc: 0.48046875\n",
      "15 40 loss: 1.570345401763916 acc: 0.4251953\n",
      "15 80 loss: 1.5021212100982666 acc: 0.42207032\n",
      "15 120 loss: 1.5859639644622803 acc: 0.41748047\n",
      "15 160 loss: 1.5495483875274658 acc: 0.42285156\n",
      "test acc: 0.4172\n",
      "16 0 loss: 1.4838244915008545 acc: 0.421875\n",
      "16 40 loss: 1.5872983932495117 acc: 0.42490235\n",
      "16 80 loss: 1.5341205596923828 acc: 0.42646486\n",
      "16 120 loss: 1.5931061506271362 acc: 0.4256836\n",
      "16 160 loss: 1.5577378273010254 acc: 0.42958984\n",
      "test acc: 0.4237\n",
      "17 0 loss: 1.4437202215194702 acc: 0.4375\n",
      "17 40 loss: 1.5449944734573364 acc: 0.43193358\n",
      "17 80 loss: 1.560814619064331 acc: 0.4256836\n",
      "17 120 loss: 1.4374277591705322 acc: 0.43427736\n",
      "17 160 loss: 1.4754865169525146 acc: 0.43554688\n",
      "test acc: 0.424\n",
      "18 0 loss: 1.5628576278686523 acc: 0.3984375\n",
      "18 40 loss: 1.5160839557647705 acc: 0.43798828\n",
      "18 80 loss: 1.528059959411621 acc: 0.43623048\n",
      "18 120 loss: 1.5132441520690918 acc: 0.4357422\n",
      "18 160 loss: 1.5491468906402588 acc: 0.4375\n",
      "test acc: 0.4299\n",
      "19 0 loss: 1.4867101907730103 acc: 0.44140625\n",
      "19 40 loss: 1.498475193977356 acc: 0.43974608\n",
      "19 80 loss: 1.6328346729278564 acc: 0.44580078\n",
      "19 120 loss: 1.4401984214782715 acc: 0.4366211\n",
      "19 160 loss: 1.5534100532531738 acc: 0.43623048\n",
      "test acc: 0.4285\n",
      "20 0 loss: 1.5068248510360718 acc: 0.45703125\n",
      "20 40 loss: 1.4820863008499146 acc: 0.4408203\n",
      "20 80 loss: 1.4397512674331665 acc: 0.44257814\n",
      "20 120 loss: 1.6220417022705078 acc: 0.4392578\n",
      "20 160 loss: 1.5070137977600098 acc: 0.44013673\n",
      "test acc: 0.443\n",
      "21 0 loss: 1.545466661453247 acc: 0.44921875\n",
      "21 40 loss: 1.4901481866836548 acc: 0.4491211\n",
      "21 80 loss: 1.4946537017822266 acc: 0.44902343\n",
      "21 120 loss: 1.5099860429763794 acc: 0.4386719\n",
      "21 160 loss: 1.5257505178451538 acc: 0.44267577\n",
      "test acc: 0.4355\n",
      "22 0 loss: 1.4374477863311768 acc: 0.47265625\n",
      "22 40 loss: 1.4843783378601074 acc: 0.45234376\n",
      "22 80 loss: 1.4446396827697754 acc: 0.44023436\n",
      "22 120 loss: 1.5780413150787354 acc: 0.44511718\n",
      "22 160 loss: 1.5046253204345703 acc: 0.44472656\n",
      "test acc: 0.4439\n",
      "23 0 loss: 1.368227243423462 acc: 0.4921875\n",
      "23 40 loss: 1.3997759819030762 acc: 0.44628906\n",
      "23 80 loss: 1.506831407546997 acc: 0.45166016\n",
      "23 120 loss: 1.3748266696929932 acc: 0.4515625\n",
      "23 160 loss: 1.5513559579849243 acc: 0.45625\n",
      "test acc: 0.4471\n",
      "24 0 loss: 1.4750326871871948 acc: 0.4609375\n",
      "24 40 loss: 1.4192655086517334 acc: 0.4513672\n",
      "24 80 loss: 1.422320008277893 acc: 0.4548828\n",
      "24 120 loss: 1.539594292640686 acc: 0.453125\n",
      "24 160 loss: 1.5298131704330444 acc: 0.45390624\n",
      "test acc: 0.4489\n",
      "25 0 loss: 1.5126676559448242 acc: 0.41796875\n",
      "25 40 loss: 1.4698346853256226 acc: 0.45371094\n",
      "25 80 loss: 1.4523828029632568 acc: 0.45439452\n",
      "25 120 loss: 1.4807381629943848 acc: 0.44746095\n",
      "25 160 loss: 1.4699846506118774 acc: 0.46357423\n",
      "test acc: 0.4565\n",
      "26 0 loss: 1.4355909824371338 acc: 0.46484375\n",
      "26 40 loss: 1.5371203422546387 acc: 0.45419922\n",
      "26 80 loss: 1.4772353172302246 acc: 0.45742187\n",
      "26 120 loss: 1.515533208847046 acc: 0.4658203\n",
      "26 160 loss: 1.5417131185531616 acc: 0.45898438\n",
      "test acc: 0.4549\n",
      "27 0 loss: 1.4196441173553467 acc: 0.43359375\n",
      "27 40 loss: 1.5040345191955566 acc: 0.46552736\n",
      "27 80 loss: 1.4984623193740845 acc: 0.459375\n",
      "27 120 loss: 1.5900568962097168 acc: 0.4626953\n",
      "27 160 loss: 1.5982178449630737 acc: 0.46611327\n",
      "test acc: 0.4538\n",
      "28 0 loss: 1.5612105131149292 acc: 0.41015625\n",
      "28 40 loss: 1.5056577920913696 acc: 0.45820314\n",
      "28 80 loss: 1.3835504055023193 acc: 0.46142578\n",
      "28 120 loss: 1.3836828470230103 acc: 0.46923828\n",
      "28 160 loss: 1.400566577911377 acc: 0.4602539\n",
      "test acc: 0.4599\n",
      "29 0 loss: 1.4005322456359863 acc: 0.52734375\n",
      "29 40 loss: 1.4324145317077637 acc: 0.46523437\n",
      "29 80 loss: 1.6332639455795288 acc: 0.46171874\n",
      "29 120 loss: 1.4496432542800903 acc: 0.4671875\n",
      "29 160 loss: 1.53708016872406 acc: 0.45957032\n",
      "test acc: 0.4617\n",
      "30 0 loss: 1.4670602083206177 acc: 0.46484375\n",
      "30 40 loss: 1.4595218896865845 acc: 0.45507812\n",
      "30 80 loss: 1.4679839611053467 acc: 0.47070312\n",
      "30 120 loss: 1.4320001602172852 acc: 0.46777344\n",
      "30 160 loss: 1.3779637813568115 acc: 0.46679688\n",
      "test acc: 0.468\n",
      "31 0 loss: 1.4099695682525635 acc: 0.46484375\n",
      "31 40 loss: 1.3331527709960938 acc: 0.47109374\n",
      "31 80 loss: 1.4738128185272217 acc: 0.47011718\n",
      "31 120 loss: 1.412667989730835 acc: 0.46601564\n",
      "31 160 loss: 1.364375114440918 acc: 0.46816406\n",
      "test acc: 0.4664\n",
      "32 0 loss: 1.4114956855773926 acc: 0.515625\n",
      "32 40 loss: 1.4996824264526367 acc: 0.47197264\n",
      "32 80 loss: 1.3627874851226807 acc: 0.4678711\n",
      "32 120 loss: 1.343623161315918 acc: 0.47636718\n",
      "32 160 loss: 1.4387072324752808 acc: 0.46396485\n",
      "test acc: 0.4672\n",
      "33 0 loss: 1.348059058189392 acc: 0.4609375\n",
      "33 40 loss: 1.4221718311309814 acc: 0.4708008\n",
      "33 80 loss: 1.4339230060577393 acc: 0.4756836\n",
      "33 120 loss: 1.3492265939712524 acc: 0.47177735\n",
      "33 160 loss: 1.3355070352554321 acc: 0.4745117\n",
      "test acc: 0.4672\n",
      "34 0 loss: 1.3543956279754639 acc: 0.5078125\n",
      "34 40 loss: 1.4956660270690918 acc: 0.47783202\n",
      "34 80 loss: 1.3420891761779785 acc: 0.4708008\n",
      "34 120 loss: 1.468745231628418 acc: 0.47802734\n",
      "34 160 loss: 1.4218473434448242 acc: 0.4711914\n",
      "test acc: 0.4715\n",
      "35 0 loss: 1.496473789215088 acc: 0.46875\n",
      "35 40 loss: 1.4926064014434814 acc: 0.47792968\n",
      "35 80 loss: 1.3920392990112305 acc: 0.47822267\n",
      "35 120 loss: 1.3416521549224854 acc: 0.47460938\n",
      "35 160 loss: 1.4326672554016113 acc: 0.48115236\n",
      "test acc: 0.4769\n",
      "36 0 loss: 1.4218714237213135 acc: 0.48046875\n",
      "36 40 loss: 1.4301490783691406 acc: 0.4774414\n",
      "36 80 loss: 1.4234580993652344 acc: 0.48085937\n",
      "36 120 loss: 1.4836534261703491 acc: 0.4836914\n",
      "36 160 loss: 1.4916353225708008 acc: 0.47822267\n",
      "test acc: 0.4706\n",
      "37 0 loss: 1.469912052154541 acc: 0.5\n",
      "37 40 loss: 1.4397691488265991 acc: 0.48173827\n",
      "37 80 loss: 1.3644709587097168 acc: 0.4783203\n",
      "37 120 loss: 1.4090416431427002 acc: 0.47998047\n",
      "37 160 loss: 1.3943376541137695 acc: 0.48535156\n",
      "test acc: 0.4788\n",
      "38 0 loss: 1.3894364833831787 acc: 0.4921875\n",
      "38 40 loss: 1.3845140933990479 acc: 0.47763672\n",
      "38 80 loss: 1.5369741916656494 acc: 0.48671874\n",
      "38 120 loss: 1.389530897140503 acc: 0.48408204\n",
      "38 160 loss: 1.4209702014923096 acc: 0.47646484\n",
      "test acc: 0.4806\n",
      "39 0 loss: 1.379316806793213 acc: 0.515625\n",
      "39 40 loss: 1.3435949087142944 acc: 0.48466796\n",
      "39 80 loss: 1.3694586753845215 acc: 0.48330078\n",
      "39 120 loss: 1.3631446361541748 acc: 0.48564452\n",
      "39 160 loss: 1.4035229682922363 acc: 0.4885742\n",
      "test acc: 0.476\n",
      "40 0 loss: 1.3839967250823975 acc: 0.48046875\n",
      "40 40 loss: 1.3669438362121582 acc: 0.4807617\n",
      "40 80 loss: 1.4449880123138428 acc: 0.48652345\n",
      "40 120 loss: 1.3051053285598755 acc: 0.48876953\n",
      "40 160 loss: 1.3382282257080078 acc: 0.4873047\n",
      "test acc: 0.4785\n",
      "41 0 loss: 1.3377346992492676 acc: 0.4921875\n",
      "41 40 loss: 1.2646996974945068 acc: 0.4921875\n",
      "41 80 loss: 1.3236925601959229 acc: 0.48662108\n",
      "41 120 loss: 1.3103337287902832 acc: 0.48544922\n",
      "41 160 loss: 1.3942651748657227 acc: 0.4801758\n",
      "test acc: 0.4858\n",
      "42 0 loss: 1.4119682312011719 acc: 0.43359375\n",
      "42 40 loss: 1.3862805366516113 acc: 0.47607422\n",
      "42 80 loss: 1.4258439540863037 acc: 0.48212892\n",
      "42 120 loss: 1.5041468143463135 acc: 0.50517577\n",
      "42 160 loss: 1.3766603469848633 acc: 0.50205076\n",
      "test acc: 0.4829\n",
      "43 0 loss: 1.5361912250518799 acc: 0.41796875\n",
      "43 40 loss: 1.339104175567627 acc: 0.49853516\n",
      "43 80 loss: 1.4382743835449219 acc: 0.4794922\n",
      "43 120 loss: 1.2861402034759521 acc: 0.49609375\n",
      "43 160 loss: 1.3824284076690674 acc: 0.4988281\n",
      "test acc: 0.4871\n",
      "44 0 loss: 1.477515697479248 acc: 0.5078125\n",
      "44 40 loss: 1.2908066511154175 acc: 0.49023438\n",
      "44 80 loss: 1.3408451080322266 acc: 0.4959961\n",
      "44 120 loss: 1.4367856979370117 acc: 0.49833983\n",
      "44 160 loss: 1.4162437915802002 acc: 0.49023438\n",
      "test acc: 0.4846\n",
      "45 0 loss: 1.4047009944915771 acc: 0.4453125\n",
      "45 40 loss: 1.379981279373169 acc: 0.49375\n",
      "45 80 loss: 1.5336719751358032 acc: 0.49082032\n",
      "45 120 loss: 1.3737292289733887 acc: 0.48242188\n",
      "45 160 loss: 1.338465929031372 acc: 0.50078124\n",
      "test acc: 0.4847\n",
      "46 0 loss: 1.3840137720108032 acc: 0.5\n",
      "46 40 loss: 1.3894850015640259 acc: 0.49248046\n",
      "46 80 loss: 1.4380147457122803 acc: 0.49804688\n",
      "46 120 loss: 1.4702730178833008 acc: 0.50273436\n",
      "46 160 loss: 1.3964619636535645 acc: 0.49140626\n",
      "test acc: 0.4942\n",
      "47 0 loss: 1.399257779121399 acc: 0.5078125\n",
      "47 40 loss: 1.271475076675415 acc: 0.49951172\n",
      "47 80 loss: 1.4971078634262085 acc: 0.49501953\n",
      "47 120 loss: 1.3775991201400757 acc: 0.49326172\n",
      "47 160 loss: 1.4337656497955322 acc: 0.49785155\n",
      "test acc: 0.4898\n",
      "48 0 loss: 1.2898802757263184 acc: 0.53515625\n",
      "48 40 loss: 1.3800618648529053 acc: 0.49609375\n",
      "48 80 loss: 1.357391119003296 acc: 0.48183593\n",
      "48 120 loss: 1.4042142629623413 acc: 0.5005859\n",
      "48 160 loss: 1.3970308303833008 acc: 0.5024414\n",
      "test acc: 0.4908\n",
      "49 0 loss: 1.414914846420288 acc: 0.4921875\n",
      "49 40 loss: 1.3385212421417236 acc: 0.51035154\n",
      "49 80 loss: 1.3002612590789795 acc: 0.4876953\n",
      "49 120 loss: 1.3513679504394531 acc: 0.4958008\n",
      "49 160 loss: 1.2852494716644287 acc: 0.50214845\n",
      "test acc: 0.4976\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 247914<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd6e62c78d0449bafdd299d7e845474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/home/tuanho/Workspace/ms/math/Assignment/wandb/run-20210727_000005-1z3sd29w/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/home/tuanho/Workspace/ms/math/Assignment/wandb/run-20210727_000005-1z3sd29w/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>acc</td><td>0.50215</td></tr><tr><td>loss</td><td>1.28525</td></tr><tr><td>_runtime</td><td>1169</td></tr><tr><td>_timestamp</td><td>1627319974</td></tr><tr><td>_step</td><td>299</td></tr><tr><td>val_acc</td><td>0.4976</td></tr><tr><td>val_loss</td><td>1.35028</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>acc</td><td>▁▄▅▅▆▅▆▆▆▆▆▆▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇██▇█████████</td></tr><tr><td>loss</td><td>█▅▅▅▄▄▄▃▃▃▃▃▂▃▂▂▂▃▁▂▂▃▃▃▂▁▁▂▂▂▂▁▁▂▂▂▁▂▂▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▂▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇███████</td></tr><tr><td>val_loss</td><td>█▆▆▅▆▅▅▅▄▂▄▄▅▂▁▄▁▃▆▃▃▃▂▁▁▄▃▂▃▃▃▁▂▂▄▃▁▃▁▂</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">faithful-brook-19</strong>: <a href=\"https://wandb.ai/tuanhoict2020/Assignment/runs/1z3sd29w\" target=\"_blank\">https://wandb.ai/tuanhoict2020/Assignment/runs/1z3sd29w</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load data\n",
    "mnist = tf.keras.datasets.cifar10\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = normalize(x_train, x_test)\n",
    "\n",
    "train_loader = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "train_loader = train_loader.map(prepare_cifar).shuffle(50000).batch(256)\n",
    "\n",
    "test_loader = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_loader = test_loader.map(prepare_cifar).shuffle(10000).batch(256)\n",
    "\n",
    "#Build model\n",
    "if params.model == \"miniresnet\":\n",
    "    model = MiniResnet()\n",
    "else:\n",
    "    model = LeNet()\n",
    "criterion = tf.losses.CategoricalCrossentropy()\n",
    "# criterion = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "if params.optimizer == \"adam\":\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD')\n",
    "metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "#Train model\n",
    "epochs = 50\n",
    "batch_size = 512\n",
    "n_batches = x_train.shape[0] // batch_size\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "train_acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "test_loss = tf.keras.metrics.Mean()\n",
    "test_acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "wandb.init()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        y = tf.squeeze(y, axis=1)\n",
    "        y = tf.one_hot(y, depth=10) # [b, 10]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            loss = criterion(y, logits)\n",
    "            metric.update_state(y, logits)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        grads = [ tf.clip_by_norm(g, 15) for g in grads]\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if step % 40 == 0:\n",
    "            print(epoch, step, 'loss:', float(loss), 'acc:', metric.result().numpy())\n",
    "            acc = metric.result().numpy()\n",
    "            loss = float(loss)\n",
    "            wandb.log({'acc': acc, 'loss': loss})\n",
    "            metric.reset_states()\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "        for x, y in test_loader:\n",
    "            y = tf.squeeze(y, axis=1)\n",
    "            y = tf.one_hot(y, depth=10)\n",
    "\n",
    "            logits = model.predict(x)\n",
    "            # be careful, these functions can accept y as [b] without warnning.\n",
    "            metric.update_state(y, logits)\n",
    "        print('test acc:', metric.result().numpy())\n",
    "        val_acc = metric.result().numpy()\n",
    "        val_loss = float(loss)  \n",
    "        wandb.log({'val_acc': val_acc, 'val_loss': val_loss})\n",
    "\n",
    "        metric.reset_states()\n",
    "# Mark the run as finished\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
